{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb131efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixtime</th>\n",
       "      <th>datetime</th>\n",
       "      <th>temperature (C)</th>\n",
       "      <th>pH</th>\n",
       "      <th>dO</th>\n",
       "      <th>red_intensity</th>\n",
       "      <th>green_intensity</th>\n",
       "      <th>blue_intensity</th>\n",
       "      <th>color_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1744741273</td>\n",
       "      <td>2025-04-15 18:21:13.543205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.343</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1744741274</td>\n",
       "      <td>2025-04-15 18:21:14.532573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.343</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744741275</td>\n",
       "      <td>2025-04-15 18:21:15.519331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.344</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744741276</td>\n",
       "      <td>2025-04-15 18:21:16.507702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.344</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744741277</td>\n",
       "      <td>2025-04-15 18:21:17.494259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.343</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693024</th>\n",
       "      <td>1745406761</td>\n",
       "      <td>2025-04-23 11:12:41.293880</td>\n",
       "      <td>28.98</td>\n",
       "      <td>7.064</td>\n",
       "      <td>7.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693025</th>\n",
       "      <td>1745406762</td>\n",
       "      <td>2025-04-23 11:12:42.279130</td>\n",
       "      <td>28.97</td>\n",
       "      <td>7.063</td>\n",
       "      <td>7.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693026</th>\n",
       "      <td>1745406763</td>\n",
       "      <td>2025-04-23 11:12:43.266820</td>\n",
       "      <td>28.97</td>\n",
       "      <td>7.062</td>\n",
       "      <td>7.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693027</th>\n",
       "      <td>1745406764</td>\n",
       "      <td>2025-04-23 11:12:44.255699</td>\n",
       "      <td>28.98</td>\n",
       "      <td>7.061</td>\n",
       "      <td>7.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693028</th>\n",
       "      <td>1745406765</td>\n",
       "      <td>2025-04-23 11:12:45.241069</td>\n",
       "      <td>28.98</td>\n",
       "      <td>7.060</td>\n",
       "      <td>7.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>693029 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          unixtime                    datetime  temperature (C)     pH    dO  \\\n",
       "0       1744741273  2025-04-15 18:21:13.543205              NaN  7.343  0.00   \n",
       "1       1744741274  2025-04-15 18:21:14.532573              NaN  7.343  0.00   \n",
       "2       1744741275  2025-04-15 18:21:15.519331              NaN  7.344  0.00   \n",
       "3       1744741276  2025-04-15 18:21:16.507702              NaN  7.344  0.00   \n",
       "4       1744741277  2025-04-15 18:21:17.494259              NaN  7.343  0.00   \n",
       "...            ...                         ...              ...    ...   ...   \n",
       "693024  1745406761  2025-04-23 11:12:41.293880            28.98  7.064  7.78   \n",
       "693025  1745406762  2025-04-23 11:12:42.279130            28.97  7.063  7.78   \n",
       "693026  1745406763  2025-04-23 11:12:43.266820            28.97  7.062  7.78   \n",
       "693027  1745406764  2025-04-23 11:12:44.255699            28.98  7.061  7.79   \n",
       "693028  1745406765  2025-04-23 11:12:45.241069            28.98  7.060  7.79   \n",
       "\n",
       "        red_intensity  green_intensity  blue_intensity  color_combined  \n",
       "0                 NaN              NaN             NaN             NaN  \n",
       "1                 NaN              NaN             NaN             NaN  \n",
       "2                 NaN              NaN             NaN             NaN  \n",
       "3                 NaN              NaN             NaN             NaN  \n",
       "4                 NaN              NaN             NaN             NaN  \n",
       "...               ...              ...             ...             ...  \n",
       "693024            NaN              NaN             NaN             NaN  \n",
       "693025            NaN              NaN             NaN             NaN  \n",
       "693026            NaN              NaN             NaN             NaN  \n",
       "693027            NaN              NaN             NaN             NaN  \n",
       "693028            NaN              NaN             NaN             NaN  \n",
       "\n",
       "[693029 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_combined = pd.read_csv('cyano_culture_datasets/04-15-2025 culture/combined_dataset.csv')\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216b01d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimpute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KNNImputer\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load and examine the dataset\n",
    "df = pd.read_csv('cyano_culture_datasets/04-15-2025 culture/combined_dataset.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# 2. Convert datetime to proper format and set as index for time-based operations\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# 3. Perform nearest neighbor imputation based on unixtime\n",
    "# First, we'll focus on the columns we're interested in\n",
    "features_of_interest = ['temperature (C)', 'pH', 'dO', 'green_intensity']\n",
    "df_subset = df[['unixtime'] + features_of_interest].copy()\n",
    "\n",
    "# Before imputation, let's count the missing values\n",
    "print(\"Missing values before imputation:\")\n",
    "print(df_subset[features_of_interest].isna().sum())\n",
    "\n",
    "# Prepare for KNN imputation - we'll use unixtime as a feature for finding nearest neighbors\n",
    "# First normalize unixtime to avoid it dominating the distance calculation\n",
    "scaler = StandardScaler()\n",
    "unixtime_scaled = scaler.fit_transform(df_subset[['unixtime']])\n",
    "\n",
    "# Create a dataframe with scaled unixtime and the features we want to impute\n",
    "imputation_df = pd.DataFrame(unixtime_scaled, columns=['unixtime_scaled'])\n",
    "for col in features_of_interest:\n",
    "    imputation_df[col] = df_subset[col]\n",
    "\n",
    "# Perform KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputed_array = imputer.fit_transform(imputation_df)\n",
    "\n",
    "# Convert back to dataframe\n",
    "imputed_df = pd.DataFrame(imputed_array, columns=imputation_df.columns)\n",
    "\n",
    "# Replace the original features with imputed values (except the scaled unixtime)\n",
    "for col in features_of_interest:\n",
    "    df_subset[col] = imputed_df[col]\n",
    "\n",
    "# Put the imputed values back into the original dataframe\n",
    "for col in features_of_interest:\n",
    "    df[col] = df_subset[col]\n",
    "\n",
    "# Check missing values after imputation\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df[features_of_interest].isna().sum())\n",
    "\n",
    "# 4. Compute hourly summary statistics for temperature, pH, and dO\n",
    "# First, let's identify rows with non-NaN green_intensity values (before imputation)\n",
    "# We'll use these as hourly markers\n",
    "\n",
    "# Create a boolean mask for rows with non-NaN green_intensity in original data\n",
    "original_green_intensity = pd.read_csv('cyano_culture_datasets/04-15-2025 culture/combined_dataset.csv')['green_intensity']\n",
    "valid_green_intensity = ~original_green_intensity.isna()\n",
    "\n",
    "# Get indices of rows with valid green_intensity\n",
    "valid_indices = np.where(valid_green_intensity)[0]\n",
    "\n",
    "# Function to compute summary statistics for a segment of data\n",
    "def compute_segment_stats(segment, feature):\n",
    "    if len(segment) == 0:\n",
    "        return {\n",
    "            f'{feature}_mean': np.nan,\n",
    "            f'{feature}_min': np.nan,\n",
    "            f'{feature}_max': np.nan,\n",
    "            f'{feature}_std': np.nan,\n",
    "            f'{feature}_range': np.nan,\n",
    "            f'{feature}_rate_of_change': np.nan\n",
    "        }\n",
    "    \n",
    "    # Basic statistics\n",
    "    stats = {\n",
    "        f'{feature}_mean': segment.mean(),\n",
    "        f'{feature}_min': segment.min(),\n",
    "        f'{feature}_max': segment.max(),\n",
    "        f'{feature}_std': segment.std(),\n",
    "        f'{feature}_range': segment.max() - segment.min()\n",
    "    }\n",
    "    \n",
    "    # Rate of change (if there are multiple points)\n",
    "    if len(segment) > 1:\n",
    "        # Simple linear regression to get slope\n",
    "        x = np.arange(len(segment))\n",
    "        y = segment.values\n",
    "        if np.std(y) > 0:  # Avoid division by zero\n",
    "            slope = np.polyfit(x, y, 1)[0]\n",
    "            stats[f'{feature}_rate_of_change'] = slope\n",
    "        else:\n",
    "            stats[f'{feature}_rate_of_change'] = 0\n",
    "    else:\n",
    "        stats[f'{feature}_rate_of_change'] = 0\n",
    "        \n",
    "    return stats\n",
    "\n",
    "# Create a new dataframe to store hourly summary metrics\n",
    "hourly_summary = []\n",
    "\n",
    "# For each valid green_intensity reading, compute summary statistics for preceding hour\n",
    "for i in range(len(valid_indices)):\n",
    "    current_idx = valid_indices[i]\n",
    "    \n",
    "    # Define start index (previous valid reading or beginning of dataset)\n",
    "    if i > 0:\n",
    "        start_idx = valid_indices[i-1] + 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    \n",
    "    # Get segment of data\n",
    "    segment = df.iloc[start_idx:current_idx+1]\n",
    "    \n",
    "    # Record the timestamp for this summary\n",
    "    summary_row = {\n",
    "        'unixtime': df.iloc[current_idx]['unixtime'],\n",
    "        'datetime': df.iloc[current_idx]['datetime'],\n",
    "        'green_intensity': df.iloc[current_idx]['green_intensity']\n",
    "    }\n",
    "    \n",
    "    # Compute statistics for each feature\n",
    "    for feature in ['temperature (C)', 'pH', 'dO']:\n",
    "        feature_stats = compute_segment_stats(segment[feature], feature)\n",
    "        summary_row.update(feature_stats)\n",
    "    \n",
    "    hourly_summary.append(summary_row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "hourly_df = pd.DataFrame(hourly_summary)\n",
    "\n",
    "# Display the resulting hourly summary dataframe\n",
    "print(\"\\nHourly summary dataframe:\")\n",
    "print(hourly_df.head())\n",
    "print(f\"Hourly summary shape: {hourly_df.shape}\")\n",
    "\n",
    "# Save the processed data\n",
    "df.to_csv('imputed_dataset.csv', index=False)\n",
    "hourly_df.to_csv('hourly_summary.csv', index=False)\n",
    "\n",
    "# 5. Visualization to verify imputation and summary\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot imputed temperature\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(df['datetime'], df['temperature (C)'])\n",
    "plt.title('Imputed Temperature')\n",
    "plt.ylabel('Temperature (C)')\n",
    "\n",
    "# Plot imputed pH\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(df['datetime'], df['pH'])\n",
    "plt.title('Imputed pH')\n",
    "plt.ylabel('pH')\n",
    "\n",
    "# Plot imputed dO\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(df['datetime'], df['dO'])\n",
    "plt.title('Imputed dO')\n",
    "plt.ylabel('dO')\n",
    "\n",
    "# Plot green_intensity\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(df['datetime'], df['green_intensity'])\n",
    "plt.title('Green Intensity')\n",
    "plt.ylabel('Green Intensity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('imputed_data_visualization.png')\n",
    "\n",
    "# Plot summary statistics\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Temperature summary\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(hourly_df['datetime'], hourly_df['temperature (C)_mean'])\n",
    "plt.title('Hourly Mean Temperature')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(hourly_df['datetime'], hourly_df['temperature (C)_min'], 'g-', \n",
    "         hourly_df['datetime'], hourly_df['temperature (C)_max'], 'r-')\n",
    "plt.title('Hourly Min/Max Temperature')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(['Min', 'Max'])\n",
    "\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.plot(hourly_df['datetime'], hourly_df['temperature (C)_rate_of_change'])\n",
    "plt.title('Temperature Rate of Change')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# pH summary\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.plot(hourly_df['datetime'], hourly_df['pH_mean'])\n",
    "plt.title('Hourly Mean pH')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.plot(hourly_df['datetime'], hourly_df['pH_min'], 'g-',\n",
    "         hourly_df['datetime'], hourly_df['pH_max'], 'r-')\n",
    "plt.title('Hourly Min/Max pH')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(['Min', 'Max'])\n",
    "\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.plot(hourly_df['datetime'], hourly_df['pH_rate_of_change'])\n",
    "plt.title('pH Rate of Change')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# dO summary\n",
    "plt.subplot(3, 3, 7)\n",
    "plt.plot(hourly_df['datetime'], hourly_df['dO_mean'])\n",
    "plt.title('Hourly Mean dO')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(3, 3, 8)\n",
    "plt.plot(hourly_df['datetime'], hourly_df['dO_min'], 'g-',\n",
    "         hourly_df['datetime'], hourly_df['dO_max'], 'r-')\n",
    "plt.title('Hourly Min/Max dO')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(['Min', 'Max'])\n",
    "\n",
    "plt.subplot(3, 3, 9)\n",
    "plt.plot(hourly_df['datetime'], hourly_df['dO_rate_of_change'])\n",
    "plt.title('dO Rate of Change')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hourly_summary_visualization.png')\n",
    "\n",
    "print(\"\\nProcessing complete. Files saved: imputed_dataset.csv, hourly_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002dc57",
   "metadata": {},
   "source": [
    "# Okay the big question is whether or not we want to do Data Imputation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4d2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "856a088b",
   "metadata": {},
   "source": [
    "# So we'll wanna do some sort of normalization probably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d172bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# normalize the data using standardscaler\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      3\u001b[39m scaler = StandardScaler()\n\u001b[32m      4\u001b[39m df_combined.iloc[:, \u001b[32m3\u001b[39m:] = scaler.fit_transform(df_combined.iloc[:, \u001b[32m1\u001b[39m:])\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# normalize the data using standardscaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_combined.iloc[:, 3:] = scaler.fit_transform(df_combined.iloc[:, 1:])\n",
    "df_combined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_city",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
